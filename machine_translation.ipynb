{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation Project\n",
    "A deep neural network that functions as part of an end-to-end machine translation pipeline\n",
    "\n",
    "The completed pipeline should accept English text as input and return the french translation as output. \n",
    "The big picture of this project will go in this order :) \n",
    "\n",
    "- **Preprocess** - It'll convert text to sequence of integers.\n",
    "- **Models** Create models which accepts a sequence of integers as input and returns a probability distribution over possible translations. After learning about the basic types of neural networks that are often used for machine translation, One can engage in their own investigations, to design their own model!\n",
    "- **Prediction** Run the model on English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport helper, tests\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections \n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "import project_tests as tests\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam \n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying access to the GPU\n",
    "The following test applies only if one expects to be using a GPU, e.g., while running in a School Workspace or using an AWS instance with GPU support. Run the next cell, and verify that the device_type is \"GPU\".\n",
    "- If the device is not GPU & you are running from a Udacity Workspace, then save your workspace with the icon at the top, then click \"enable\" at the bottom of the workspace.\n",
    "- If the device is not GPU & you are running from an AWS instance, then refer to the cloud computing instructions in the classroom to verify your setup steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9120074877286856748\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "This dataset will be used to train and evaluate the pipeline. This dataset contains a small vocabulary. This dataset will allow the model to be trained in a reasonable amount of time. \n",
    "### Load Data\n",
    "The data is located in `data/small_vocab_en` and `data/small_vocab_fr`. The `small_vocab_en` file contains English sentences with their French translations in the `small_vocab_fr` file. Load the English and French data from these files from running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Loading english data\n",
    "english_sentences = helper.load_data('data/small_vocab_en')\n",
    "# Loading french data\n",
    "french_sentences = helper.load_data('data/small_vocab_fr')\n",
    "\n",
    "print('Dataset Loaded')\n",
    "print(type(english_sentences))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files \n",
    "Each line in `small_vocab_en` contains an English sentence with the respective translation in each line of `small_vocab_fr`. View the first two lines from each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line1: new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "small_vocab_fr Line1: new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "small_vocab_en Line2: the united states is usually chilly during july , and it is usually freezing in november .\n",
      "small_vocab_fr Line2: les Ã©tats-unis est gÃ©nÃ©ralement froid en juillet , et il gÃ¨le habituellement en novembre .\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('small_vocab_en Line{}: {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line{}: {}'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the sentences, One can see they have been preprocessed already. The puncuations have been delimited using spaces. All the text have been converted to lowercase. This should save some time, but the text requires more preprocessing.\n",
    "### Vocabulary \n",
    "The complexity of the problem is determined by the complexity of the vocabulary.  A more complex vocabulary is a more complex problem.  Let's look at the complexity of the dataset it'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823250 English words.\n",
      "227 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
      "\n",
      "1961295 French words.\n",
      "355 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, _Alice's Adventures in Wonderland_ contains 2,766 unique words of a total of 15,500 words.\n",
    "## Preprocess\n",
    "For this project, It won't use text data as input to the model. Instead, it'll convert the text into sequences of integers using the following preprocess methods:\n",
    "1. Tokenize the words into ids\n",
    "2. Add padding to make all the sequences the same length.\n",
    "\n",
    "Time to start preprocessing the data...\n",
    "### Tokenize (1750)\n",
    "For a neural network to predict on text data, it first has to be turned into data it can understand. Text data like \"dog\" is a sequence of ASCII character encodings.  Since a neural network is a series of multiplication and addition operations, the input data needs to be number(s).\n",
    "\n",
    "One can turn each character into a number or each word into a number.  These are called character and word ids, respectively.  Character ids are used for character level models that generate text predictions for each character.  A word level model uses word ids that generate text predictions for each word.  Word level models tend to learn better, since they are lower in complexity, so it'll use those.\n",
    "\n",
    "Below is a dsiplay of executing each sentence into a sequence of words ids using Keras's [`Tokenizer`](https://keras.io/preprocessing/text/#tokenizer) function. One can use this function to tokenize `english_sentences` and `french_sentences` in the cell below.\n",
    "\n",
    "Running the cell will run `tokenize` on sample data and show output for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    \n",
    "    # Creates the tokeninzer\n",
    "    t = Tokenizer()\n",
    "    # Create dictionary mapping words (str) to their rank/index (int)\n",
    "    t.fit_on_texts(x)\n",
    "    # Use the tokenizer to tokenize the text\n",
    "    text_sequences = t.texts_to_sequences(x)\n",
    "    return text_sequences, t\n",
    "tests.test_tokenize(tokenize)\n",
    "\n",
    "\n",
    "# Tokenize Example output\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding(1750)\n",
    "When batching the sequence of word ids together, each sequence needs to be the same length. Since sentences are dynamic in length, One can add padding to the end of the sequences to make them the same length.\n",
    "\n",
    "It's important to build this in a way that all the English sequences have the same length and all the French sequences have the same length by adding padding to the **end** of each sequence using Keras's [`pad_sequences`](https://keras.io/preprocessing/sequence/#pad_sequences) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    \n",
    "    # If length equals None, set it to be the length of the longest sequence in x\n",
    "    if length == None:\n",
    "        length = len(max(x, key=len))\n",
    "        \n",
    "    # Using Keras's pad_sequences to pad the sequences with 0's\n",
    "    padded_sequences = pad_sequences(sequences=x, maxlen=length, padding='post', value=0)\n",
    "    \n",
    "    return padded_sequences\n",
    "\n",
    "tests.test_pad(pad)\n",
    "\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Pipeline\n",
    "The goal for this project is to build neural network architecture, Let's create a preprocess pipeline and provide the implementation of the `preprocess` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 15\n",
      "Max French sentence length: 21\n",
      "English vocabulary size: 199\n",
      "French vocabulary size: 345\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension - Splitting the data\n",
    "Using `sklearn.model_selection.train_test_split` to split the data into training and test sets.\n",
    "\n",
    "Note: `train_test_split` should be perferred on X and y data together, not seperately.\n",
    "\n",
    "For example,\n",
    "\n",
    "`train_test_split(X, y, test_size = 0.2, random_state = 42)`\n",
    "\n",
    "Do not split the data seperately (as seen as the commented out code in the below cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splits the unprocessed data into training and test sets using Sklearn ###\n",
    "\n",
    "english_sentences_train, english_sentences_test, french_sentences_train, french_sentences_test =\\\n",
    "    train_test_split(english_sentences, french_sentences, test_size=0.2, random_state=42)\n",
    "    \n",
    "    \n",
    "### Failed attempts below ###\n",
    "\n",
    "#french_sentences_train, french_sentences_test = train_test_split(french_sentences,\n",
    "                                                                 #test_size=0.2)\n",
    "    \n",
    "#english_sentences_train, english_sentences_test = train_test_split(english_sentences,\n",
    "                                                                 #test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1458806 English words in the training set.\n",
      "227 unique English words in the training set.\n",
      "10 Most common words in the English training dataset:\n",
      "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
      "\n",
      "1568964 French words in the training set.\n",
      "354 unique French words in the training set.\n",
      "10 Most common words in the French training dataset:\n",
      "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
     ]
    }
   ],
   "source": [
    "# Calculating the training set stats\n",
    "\n",
    "english_train_words_counter = collections.Counter([word for sentence in english_sentences_train for word in sentence.split()])\n",
    "french_train_words_counter = collections.Counter([word for sentence in french_sentences_train for word in sentence.split()])\n",
    "\n",
    "print('{} English words in the training set.'.format(len([word for sentence in english_sentences_train for word in sentence.split()])))\n",
    "print('{} unique English words in the training set.'.format(len(english_train_words_counter)))\n",
    "print('10 Most common words in the English training dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_train_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words in the training set.'.format(len([word for sentence in french_sentences_train for word in sentence.split()])))\n",
    "print('{} unique French words in the training set.'.format(len(french_train_words_counter)))\n",
    "print('10 Most common words in the French training dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_train_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364444 English words in the test set.\n",
      "227 unique English words in the test set.\n",
      "10 Most common words in the English test dataset:\n",
      "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
      "\n",
      "392331 French words in the test set.\n",
      "338 unique French words in the test set.\n",
      "10 Most common words in the French test dataset:\n",
      "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
     ]
    }
   ],
   "source": [
    "#2 Calculating the test set stats\n",
    "english_test_words_counter = collections.Counter([word for sentence in english_sentences_test for word in sentence.split()])\n",
    "french_test_words_counter = collections.Counter([word for sentence in french_sentences_test for word in sentence.split()])\n",
    "\n",
    "print('{} English words in the test set.'.format(len([word for sentence in english_sentences_test for word in sentence.split()])))\n",
    "print('{} unique English words in the test set.'.format(len(english_test_words_counter)))\n",
    "print('10 Most common words in the English test dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_test_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words in the test set.'.format(len([word for sentence in french_sentences_test for word in sentence.split()])))\n",
    "print('{} unique French words in the test set.'.format(len(french_test_words_counter)))\n",
    "print('10 Most common words in the French test dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_test_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the training data\n",
    "\n",
    "Using `preprocess` to process the training split of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the training split of the data\n",
    "\n",
    "preproc_english_sentences_train, preproc_french_sentences_train, english_tokenizer_train, french_tokenizer_train =\\\n",
    "    preprocess(english_sentences_train, french_sentences_train)\n",
    "    \n",
    "max_english_sequence_length_train = preproc_english_sentences_train.shape[1]\n",
    "max_french_sequence_length_train = preproc_french_sentences_train.shape[1]\n",
    "english_vocab_size_train = len(english_tokenizer_train.word_index)\n",
    "french_vocab_size_train = len(french_tokenizer_train.word_index)\n",
    "\n",
    "print('Training Data Preprocessed')\n",
    "print(\"Max English train dataset sentence length:\", max_english_sequence_length_train)\n",
    "print(\"Max French train dataset sentence length:\", max_french_sequence_length_train)\n",
    "print(\"English train dataset vocabulary size:\", english_vocab_size_train)\n",
    "print(\"French train dataset vocabulary size:\", french_vocab_size_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the training data\n",
    "\n",
    "Using `preprocess` to process the test split of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the test split of the data \n",
    "\n",
    "preproc_english_sentences_test, preproc_french_sentences_test, english_tokenizer_test, french_tokenizer_test =\\\n",
    "    preprocess(english_sentences_test, french_sentences_test)\n",
    "    \n",
    "max_english_sequence_length_test = preproc_english_sentences_test.shape[1]\n",
    "max_french_sequence_length_test = preproc_french_sentences_test.shape[1]\n",
    "english_vocab_size_test = len(english_tokenizer_test.word_index)\n",
    "french_vocab_size_test = len(french_tokenizer_test.word_index)\n",
    "\n",
    "print('Test Data Preprocessed')\n",
    "print('Max English test dataset sentence length:', max_english_sequence_length_test)\n",
    "print('Max French test dataset sentence length:', max_french_sequence_length_test)\n",
    "print('English test datset vocab size:', english_vocab_size_test)\n",
    "print('French test dataset vocab size', french_vocab_size_test)\n",
    "print(preproc_french_sentences_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "In this section, The experiment connects various neural network architectures.\n",
    "It begins by training four relatively simple architectures.\n",
    "- Model 1 is a simple RNN\n",
    "- Model 2 is a RNN with Embedding\n",
    "- Model 3 is a Bidirectional RNN\n",
    "- Model 4 is an optional Encoder-Decoder RNN\n",
    "\n",
    "After experimenting with the four simple architectures, It constructs a deeper architecture that is designed to outperform all four models.\n",
    "### Ids Back to Text\n",
    "The neural network will be translating the input to words ids, which isn't the final form that is wanted. The goal is to want the French translation.  The function `logits_to_text` will bridge the gab between the logits from the neural network to the French translation. It'll be using this function to better understand the output of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: RNN (1750)\n",
    "![RNN](images/rnn.png)\n",
    "A basic RNN model is a good baseline for sequence data.  In this model, It'll build a RNN that translates English to French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Build the layers\n",
    "    learning_rate=0.001\n",
    "    input_seq = Input(input_shape[1:])\n",
    "    rnn = GRU(64, return_sequences=True)(input_seq)\n",
    "    logits = TimeDistributed(Dense(french_vocab_size))(rnn)\n",
    "    model = Model(input_seq, Activation('softmax')(logits))\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "tests.test_simple_model(simple_model)\n",
    "\n",
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "simple_rnn_model = simple_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the simple model on the training data set\n",
    "\n",
    "Using the same model as above on the training data split to see how it performs on the training split of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Trying the simple_model on the training split of the dataset\n",
    "\n",
    "def simple_model_split(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Building the layers\n",
    "    \n",
    "    # Input layer\n",
    "    input_layer = Input(shape=input_shape[1:])\n",
    "    \n",
    "    # GRU Layer\n",
    "    x = GRU(256, return_sequences=True)(input_layer)\n",
    "    \n",
    "    # Fully-connected Layer\n",
    "    x = Dense(french_vocab_size*4, activation='relu')(x)\n",
    "    \n",
    "    # Output Layer\n",
    "    output_layer = TimeDistributed(Dense(french_vocab_size, activation='softmax'))(x)\n",
    "    \n",
    "    # Creating model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    # Compiling the model\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(lr = 0.01),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentences_train, max_french_sequence_length_train)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences_train.shape[-2], 1))\n",
    "\n",
    "# Training the model on the training set\n",
    "simple_rnn_model_split = simple_model_split(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length_train,\n",
    "    english_vocab_size_train + 1,\n",
    "    french_vocab_size_train + 1)\n",
    "\n",
    "simple_rnn_model_split.fit(tmp_x, preproc_french_sentences_train, batch_size=200, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Printing prediction(s)\n",
    "print(logits_to_text(simple_rnn_model_split.predict(tmp_x[:1])[0], french_tokenizer_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the simple model on the test data set\n",
    "\n",
    "Checking how the model trained on the training set data performs on the unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluating the model on the test set ###\n",
    "\n",
    "# Reshaping the test data \n",
    "test_x = pad(preproc_english_sentences_test, max_french_sequence_length_test)\n",
    "test_x = test_x.reshape((-1, preproc_french_sentences_train.shape[-2], 1))\n",
    "\n",
    "# Rehsaping the test label data to fit the model\n",
    "preproc_french_sentences_test = preproc_french_sentences_test.reshape((-1, preproc_french_sentences_train.shape[-2], 1))\n",
    "\n",
    "# Evaluating the model on the unseen test data\n",
    "simple_rnn_model_score = simple_rnn_model_split.evaluate(test_x, preproc_french_sentences_test, verbose=0)\n",
    "\n",
    "print(\"Simple model accuracy on unseen test dataset: {0:.2f}%\".format(simple_rnn_model_score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Embedding (Another Attempt after hours of failing the first model haha)\n",
    "![RNN](images/embedding.png)\n",
    "Earlier work done has turned the words into ids, but there's a better representation of a word.  This is called word embeddings.  An embedding is a vector representation of the word that is close to similar words in n-dimensional space, where the n represents the size of the embedding vectors.\n",
    "\n",
    "In this model, you'll create a RNN model using embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Embedding model trained on entire dataset ###\n",
    "\n",
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a RNN model using word embedding on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: \n",
    "    \n",
    "    # RNN model with embedding layer\n",
    "    input_layer = Input(shape=input_shape[1:])\n",
    "    embedding_layer = Embedding(512, english_vocab_size)(input_layer)\n",
    "    x = GRU(512, return_sequences=True)(embedding_layer)\n",
    "    x = TimeDistributed(Dense(french_vocab_size*4, activation='relu'))(x)\n",
    "    output_layer = Dense(french_vocab_size, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    # Compiling the model\n",
    "    learning_rate = 0.01\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Testing the model\n",
    "tests.test_embed_model(embed_model)\n",
    "\n",
    "# TODO: Padding the input \n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "\n",
    "# TODO: Training the neural network on the training data\n",
    "embedding_rnn_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size + 1,\n",
    "    french_vocab_size + 1)\n",
    "embedding_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "# TODO: Printing prediction(s)\n",
    "print(logits_to_text(embedding_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
